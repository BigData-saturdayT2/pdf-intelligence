{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagram saved as airflow_etl_architecture.png\n"
     ]
    }
   ],
   "source": [
    "#Airflow Archtechture Diagram\n",
    "\n",
    "from diagrams import Diagram\n",
    "from diagrams.custom import Custom\n",
    "from diagrams.gcp.storage import GCS\n",
    "from diagrams.programming.language import Python\n",
    "\n",
    "# Define custom icons (local paths or URLs can be used)\n",
    "pdf_icon = \"/Users/nishitamatlani/Documents/GitHub/assignment2-nishita/images/pdf.png\"\n",
    "watson_icon = \"/Users/nishitamatlani/Documents/GitHub/assignment2-nishita/images/ibm.png\"\n",
    "pypdf2_icon = \"/Users/nishitamatlani/Documents/GitHub/assignment2-nishita/images/python.png\"\n",
    "\n",
    "# Create the diagram with increased padding to add space above the diagram\n",
    "graph_attr = {\n",
    "    \"pad\": \"1.0\",   # Adds padding around the diagram (controls space)\n",
    "    \"fontsize\": \"22\"  # Increases font size for better readability\n",
    "}\n",
    "\n",
    "# Create and save the diagram as a PNG file (no rendering in the notebook)\n",
    "output_filename = \"airflow_etl_architecture\"\n",
    "\n",
    "# Create the diagram and save it as a file\n",
    "with Diagram(\"Airflow ETL Architecture\", filename=output_filename, show=False, direction=\"LR\", graph_attr=graph_attr):\n",
    "    # Define nodes with custom icons\n",
    "    pdf = Custom(\"PDF\", pdf_icon)\n",
    "    watson_api = Custom(\"IBM Watson API\", watson_icon)\n",
    "    gcp_bucket1 = GCS(\"GCP Bucket\")\n",
    "    gcp_bucket2 = GCS(\"GCP Bucket\")\n",
    "    \n",
    "    pypdf2 = Custom(\"PyPDF2 LIbrary\", pypdf2_icon)\n",
    "\n",
    "    # Define the workflow connections\n",
    "    gcp_bucket1 >> pdf >> watson_api >> gcp_bucket2\n",
    "    pdf >> pypdf2 >> gcp_bucket2\n",
    "\n",
    "print(f\"Diagram saved as {output_filename}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diagrams import Cluster, Diagram, Edge\n",
    "from diagrams.aws.storage import S3\n",
    "from diagrams.aws.database import RDS\n",
    "from diagrams.onprem.client import Client\n",
    "from diagrams.onprem.compute import Server\n",
    "from diagrams.programming.language import Python\n",
    "from diagrams.onprem.workflow import Airflow\n",
    "from diagrams.custom import Custom\n",
    "from diagrams.generic.compute import Rack  # Representing Docker Compose containers\n",
    "\n",
    "# Paths to your custom icons\n",
    "openai_icon_path = \"/Users/nishitamatlani/Documents/GitHub/assignment2-nishita/images/openai.png\"\n",
    "hugging_face_icon_path = \"/Users/nishitamatlani/Documents/GitHub/assignment2-nishita/images/hugging_face.png\"\n",
    "watson_icon_path = \"/Users/nishitamatlani/Documents/GitHub/assignment2-nishita/images/ibm.png\"\n",
    "streamlit_icon_path = \"/Users/nishitamatlani/Documents/GitHub/assignment2-nishita/images/streamlit.png\"\n",
    "pdf_icon_path = \"/Users/nishitamatlani/Documents/GitHub/assignment2-nishita/images/pdf.png\"\n",
    "jwt_icon_path = \"/Users/nishitamatlani/Documents/GitHub/assignment2-nishita/images/jwt.png\"  # Custom JWT icon\n",
    "\n",
    "# Create a diagram to represent the overall architecture\n",
    "with Diagram(\"Airflow ETL and Data Flow Architecture\", show=False):\n",
    "\n",
    "    # Source: Hugging Face dataset going to S3 bucket initially\n",
    "    huggingface_source = Custom(\"GAIA Dataset (Hugging Face)\", hugging_face_icon_path)\n",
    "\n",
    "    # S3 Bucket for initial storage\n",
    "    s3_bucket = S3(\"Amazon S3 Bucket\")\n",
    "\n",
    "    # ETL Orchestration and Data Extraction\n",
    "    with Cluster(\"ETL Process\"):\n",
    "        # Airflow for Orchestration\n",
    "        airflow = Airflow(\"Airflow ETL\")\n",
    "\n",
    "        # PDF processing step using custom PDF icon\n",
    "        pdf_file = Custom(\"PDF File\", pdf_icon_path)\n",
    "\n",
    "        # Custom icon for IBM Watson API\n",
    "        ibm_watson = Custom(\"IBM Watson API\", watson_icon_path)\n",
    "\n",
    "        # PyPDF for text extraction\n",
    "        pypdf2_lib = Python(\"PyPDF2 Library\")\n",
    "\n",
    "        # Target S3 bucket for storing extracted content\n",
    "        s3_target_bucket = S3(\"Amazon S3 Bucket\")\n",
    "\n",
    "        # Define ETL data flow paths with different arrow styles\n",
    "        huggingface_source >> Edge(label=\"Upload Dataset\", style=\"solid\") >> s3_bucket\n",
    "        s3_bucket >> Edge(label=\"PDF Files\", style=\"solid\") >> airflow >> Edge(label=\"Trigger ETL\", style=\"dotted\") >> pdf_file\n",
    "        pdf_file >> Edge(label=\"Text Extraction\", style=\"solid\") >> ibm_watson >> Edge(label=\"Data Transfer\", style=\"solid\") >> s3_target_bucket\n",
    "        pdf_file >> Edge(label=\"Text Extraction\", style=\"solid\") >> pypdf2_lib >> Edge(label=\"Data Transfer\", style=\"solid\") >> s3_target_bucket\n",
    "\n",
    "    # Database in Amazon RDS for data storage\n",
    "    rds_instance = RDS(\"Amazon RDS\")\n",
    "\n",
    "    # Backend Service using FastAPI\n",
    "    with Cluster(\"Backend Service\"):\n",
    "        fastapi_service = Server(\"FastAPI\")\n",
    "\n",
    "        # JWT Authentication Component between FastAPI and Streamlit\n",
    "        jwt_auth = Custom(\"JWT Authentication\", jwt_icon_path)\n",
    "\n",
    "        # Use Custom node for OpenAI\n",
    "        openai_integration = Custom(\"OpenAI\", openai_icon_path)\n",
    "\n",
    "    # Frontend Client with Streamlit\n",
    "    with Cluster(\"Frontend Service\"):\n",
    "        streamlit_app = Custom(\"Streamlit\", streamlit_icon_path)\n",
    "\n",
    "    # Data flow from S3 (ETL output) to RDS\n",
    "    s3_target_bucket >> Edge(label=\"Extracted Text\", style=\"solid\") >> rds_instance\n",
    "    s3_bucket >> Edge(label=\"Raw Data\", style=\"solid\") >> rds_instance\n",
    "\n",
    "    # FastAPI interacts with RDS and OpenAI using API calls (dashed lines)\n",
    "    rds_instance >> Edge(label=\"Data Query\", style=\"dashed\") >> fastapi_service\n",
    "    fastapi_service >> Edge(label=\"API Call\", style=\"dashed\") >> openai_integration\n",
    "\n",
    "    # JWT Authentication flow\n",
    "    fastapi_service >> Edge(label=\"JWT Verification\", style=\"dashed\") >> jwt_auth\n",
    "    jwt_auth >> Edge(label=\"Access Control\", style=\"dotted\") >> streamlit_app\n",
    "\n",
    "    # Streamlit interacts with FastAPI for data display using API calls\n",
    "    fastapi_service >> Edge(label=\"API Call\", style=\"dashed\") >> streamlit_app\n",
    "\n",
    "    # Add a separate section for Docker Compose\n",
    "    with Cluster(\"Deployment Layer\"):\n",
    "        docker_compose = Rack(\"Docker Compose\\n(Containerized Services)\")\n",
    "\n",
    "        # Docker containers for each service\n",
    "        docker_airflow = airflow - Edge(label=\"Containerized Deployment\", style=\"dashed\") - docker_compose\n",
    "        docker_fastapi = fastapi_service - Edge(label=\"Containerized Deployment\", style=\"dashed\") - docker_compose\n",
    "        docker_streamlit = streamlit_app - Edge(label=\"Containerized Deployment\", style=\"dashed\") - docker_compose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
